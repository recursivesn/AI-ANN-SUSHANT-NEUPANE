{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGDNt08Dtje/06Rno07VZ9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/recursivesn/AI-ANN-SUSHANT-NEUPANE/blob/main/Perceptron%20Traning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perceptron for 2-Input Basic Gates (AND/OR)**"
      ],
      "metadata": {
        "id": "TN4iHrargxmf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2yPtBZge0IL",
        "outputId": "42c1d3c6-efac-45a0-8f42-e78f74db7135"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Perceptron for AND Gate\n",
            "W1\tW2\tX1\tX2\tTg\tY\tError\tdW1\tdW2\tNewW1\tNewW2\n",
            "------------------------------------------------------------------------------------------\n",
            "EPOCH: 1\n",
            "0.10\t0.20\t0\t0\t0\t0\t0\t0.00\t0.00\t0.10\t0.20\n",
            "0.10\t0.20\t0\t1\t0\t0\t0\t0.00\t0.00\t0.10\t0.20\n",
            "0.10\t0.20\t1\t0\t0\t0\t0\t0.00\t0.00\t0.10\t0.20\n",
            "0.10\t0.20\t1\t1\t1\t0\t1\t0.10\t0.10\t0.20\t0.30\n",
            "------------------------------------------------------------------------------------------\n",
            "EPOCH: 2\n",
            "0.20\t0.30\t0\t0\t0\t0\t0\t0.00\t0.00\t0.20\t0.30\n",
            "0.20\t0.30\t0\t1\t0\t0\t0\t0.00\t0.00\t0.20\t0.30\n",
            "0.20\t0.30\t1\t0\t0\t0\t0\t0.00\t0.00\t0.20\t0.30\n",
            "0.20\t0.30\t1\t1\t1\t1\t0\t0.00\t0.00\t0.20\t0.30\n",
            "------------------------------------------------------------------------------------------\n",
            "Training complete after 2 epochs.\n",
            "Final weights for AND gate: W1 = 0.20, W2 = 0.30\n",
            "\n",
            "Training Perceptron for OR Gate\n",
            "W1\tW2\tX1\tX2\tTg\tY\tError\tdW1\tdW2\tNewW1\tNewW2\n",
            "------------------------------------------------------------------------------------------\n",
            "EPOCH: 1\n",
            "0.10\t0.20\t0\t0\t0\t0\t0\t0.00\t0.00\t0.10\t0.20\n",
            "0.10\t0.20\t0\t1\t1\t0\t1\t0.00\t0.10\t0.10\t0.30\n",
            "0.10\t0.30\t1\t0\t1\t0\t1\t0.10\t0.00\t0.20\t0.30\n",
            "0.20\t0.30\t1\t1\t1\t1\t0\t0.00\t0.00\t0.20\t0.30\n",
            "------------------------------------------------------------------------------------------\n",
            "EPOCH: 2\n",
            "0.20\t0.30\t0\t0\t0\t0\t0\t0.00\t0.00\t0.20\t0.30\n",
            "0.20\t0.30\t0\t1\t1\t0\t1\t0.00\t0.10\t0.20\t0.40\n",
            "0.20\t0.40\t1\t0\t1\t0\t1\t0.10\t0.00\t0.30\t0.40\n",
            "0.30\t0.40\t1\t1\t1\t1\t0\t0.00\t0.00\t0.30\t0.40\n",
            "------------------------------------------------------------------------------------------\n",
            "EPOCH: 3\n",
            "0.30\t0.40\t0\t0\t0\t0\t0\t0.00\t0.00\t0.30\t0.40\n",
            "0.30\t0.40\t0\t1\t1\t0\t1\t0.00\t0.10\t0.30\t0.50\n",
            "0.30\t0.50\t1\t0\t1\t0\t1\t0.10\t0.00\t0.40\t0.50\n",
            "0.40\t0.50\t1\t1\t1\t1\t0\t0.00\t0.00\t0.40\t0.50\n",
            "------------------------------------------------------------------------------------------\n",
            "EPOCH: 4\n",
            "0.40\t0.50\t0\t0\t0\t0\t0\t0.00\t0.00\t0.40\t0.50\n",
            "0.40\t0.50\t0\t1\t1\t1\t0\t0.00\t0.00\t0.40\t0.50\n",
            "0.40\t0.50\t1\t0\t1\t0\t1\t0.10\t0.00\t0.50\t0.50\n",
            "0.50\t0.50\t1\t1\t1\t1\t0\t0.00\t0.00\t0.50\t0.50\n",
            "------------------------------------------------------------------------------------------\n",
            "EPOCH: 5\n",
            "0.50\t0.50\t0\t0\t0\t0\t0\t0.00\t0.00\t0.50\t0.50\n",
            "0.50\t0.50\t0\t1\t1\t1\t0\t0.00\t0.00\t0.50\t0.50\n",
            "0.50\t0.50\t1\t0\t1\t1\t0\t0.00\t0.00\t0.50\t0.50\n",
            "0.50\t0.50\t1\t1\t1\t1\t0\t0.00\t0.00\t0.50\t0.50\n",
            "------------------------------------------------------------------------------------------\n",
            "Training complete after 5 epochs.\n",
            "Final weights for OR gate: W1 = 0.50, W2 = 0.50\n"
          ]
        }
      ],
      "source": [
        "def threshold(inp, th):\n",
        "    return 1 if inp >= th else 0\n",
        "\n",
        "def train_perceptron(ds, lr, th, initial_weights, gate_name):\n",
        "    w = initial_weights.copy()\n",
        "    epoch = 0\n",
        "    print(f\"\\nTraining Perceptron for {gate_name} Gate\")\n",
        "    print(\"W1\\tW2\\tX1\\tX2\\tTg\\tY\\tError\\tdW1\\tdW2\\tNewW1\\tNewW2\")\n",
        "    print(\"-\" * 90)\n",
        "\n",
        "    while True:\n",
        "        tf = False  # flag for error presence\n",
        "        epoch += 1\n",
        "        print(f\"EPOCH: {epoch}\")\n",
        "        for x1, x2, target in ds:\n",
        "            inp = x1 * w[0] + x2 * w[1]\n",
        "            y = threshold(inp, th)\n",
        "            err = target - y\n",
        "\n",
        "            if err != 0:\n",
        "                tf = True\n",
        "\n",
        "            dw1 = lr * x1 * err\n",
        "            dw2 = lr * x2 * err\n",
        "\n",
        "            print(f\"{w[0]:.2f}\\t{w[1]:.2f}\\t{x1}\\t{x2}\\t{target}\\t{y}\\t{err}\\t{dw1:.2f}\\t{dw2:.2f}\", end=\"\\t\")\n",
        "\n",
        "            w[0] += dw1\n",
        "            w[1] += dw2\n",
        "\n",
        "            print(f\"{w[0]:.2f}\\t{w[1]:.2f}\")\n",
        "        print(\"-\" * 90)\n",
        "        if not tf:\n",
        "            break\n",
        "\n",
        "    print(f\"Training complete after {epoch} epochs.\")\n",
        "    print(f\"Final weights for {gate_name} gate: W1 = {w[0]:.2f}, W2 = {w[1]:.2f}\")\n",
        "    return w\n",
        "\n",
        "# Parameters\n",
        "lr = 0.1\n",
        "th = 0.5\n",
        "initial_weights = [0.1, 0.2]\n",
        "\n",
        "# Dataset for AND gate\n",
        "ds_and = [\n",
        "    [0, 0, 0],\n",
        "    [0, 1, 0],\n",
        "    [1, 0, 0],\n",
        "    [1, 1, 1]\n",
        "]\n",
        "\n",
        "# Dataset for OR gate\n",
        "ds_or = [\n",
        "    [0, 0, 0],\n",
        "    [0, 1, 1],\n",
        "    [1, 0, 1],\n",
        "    [1, 1, 1]\n",
        "]\n",
        "\n",
        "# Train for AND gate\n",
        "weights_and = train_perceptron(ds_and, lr, th, initial_weights, \"AND\")\n",
        "\n",
        "# Train for OR gate\n",
        "weights_or = train_perceptron(ds_or, lr, th, initial_weights, \"OR\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perceptron for n-Input Basic Gates (AND/OR)**\n"
      ],
      "metadata": {
        "id": "OI7XyfiNgR0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "def linearInt(x, weights, bias):\n",
        "    return sum([x[i] * weights[i] for i in range(len(x))]) + bias\n",
        "\n",
        "def predict(x, weights, bias):\n",
        "    return 1 if linearInt(x, weights, bias) >= 0 else 0\n",
        "\n",
        "def train(X, Y, lr=0.1, max_loops=100):\n",
        "    weights = [0.0] * len(X[0])\n",
        "    bias = 0.0\n",
        "\n",
        "    for _ in range(max_loops):\n",
        "        error_found = False\n",
        "        for i in range(len(X)):\n",
        "            y_pred = predict(X[i], weights, bias)\n",
        "            error = Y[i] - y_pred\n",
        "            if error != 0:\n",
        "                for j in range(len(weights)):\n",
        "                    weights[j] += lr * error * X[i][j]\n",
        "                bias += lr * error\n",
        "                error_found = True\n",
        "        if not error_found:\n",
        "            break\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "def test_accuracy(X, Y, weights, bias):\n",
        "    correct = 0\n",
        "    for i in range(len(X)):\n",
        "        if predict(X[i], weights, bias) == Y[i]:\n",
        "            correct += 1\n",
        "    return (correct / len(X)) * 100\n",
        "\n",
        "# Run for both 3-input and 4-input gates\n",
        "for n in [3, 4]:\n",
        "    print(f\"\\n==== {n}-INPUT GATES ====\")\n",
        "    X = list(product([0, 1], repeat=n))\n",
        "\n",
        "    # AND Gate\n",
        "    Y_and = [int(all(x)) for x in X]\n",
        "    w_and, b_and = train(X, Y_and)\n",
        "    print(f\"\\nAND Gate:\")\n",
        "    print(f\"Weights: {w_and}\")\n",
        "    print(f\"Bias: {b_and}\")\n",
        "    print(f\"Accuracy: {test_accuracy(X, Y_and, w_and, b_and)}%\")\n",
        "\n",
        "    # OR Gate\n",
        "    Y_or = [int(any(x)) for x in X]\n",
        "    w_or, b_or = train(X, Y_or)\n",
        "    print(f\"\\nOR Gate:\")\n",
        "    print(f\"Weights: {w_or}\")\n",
        "    print(f\"Bias: {b_or}\")\n",
        "    print(f\"Accuracy: {test_accuracy(X, Y_or, w_or, b_or)}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff8sYXIMe_27",
        "outputId": "2d5a6c6f-09e4-4a4f-d049-612b06aa4425"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== 3-INPUT GATES ====\n",
            "\n",
            "AND Gate:\n",
            "Weights: [0.1, 0.1, 0.1]\n",
            "Bias: -0.20000000000000004\n",
            "Accuracy: 100.0%\n",
            "\n",
            "OR Gate:\n",
            "Weights: [0.1, 0.1, 0.1]\n",
            "Bias: -0.1\n",
            "Accuracy: 100.0%\n",
            "\n",
            "==== 4-INPUT GATES ====\n",
            "\n",
            "AND Gate:\n",
            "Weights: [0.4, 0.20000000000000004, 0.1, 0.1]\n",
            "Bias: -0.7999999999999999\n",
            "Accuracy: 100.0%\n",
            "\n",
            "OR Gate:\n",
            "Weights: [0.1, 0.1, 0.1, 0.1]\n",
            "Bias: -0.1\n",
            "Accuracy: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perceptron for Linear Function with 3 Features**\n"
      ],
      "metadata": {
        "id": "i4f67GiqpJdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Generate dataset: y = 2x1 + 3x2 - x3 + 5\n",
        "X = []\n",
        "Y = []\n",
        "for _ in range(10):\n",
        "    x1 = random.uniform(0, 1)\n",
        "    x2 = random.uniform(0, 1)\n",
        "    x3 = random.uniform(0, 1)\n",
        "    y = 2 * x1 + 3 * x2 - 1 * x3 + 5\n",
        "    X.append([x1, x2, x3])\n",
        "    Y.append(y)\n",
        "\n",
        "# Initialize weights and bias\n",
        "weights = [0.0, 0.0, 0.0]\n",
        "bias = 0.0\n",
        "lr = 0.01\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "    total_error = 0\n",
        "    for i in range(len(X)):\n",
        "        x = X[i]\n",
        "        y_true = Y[i]\n",
        "\n",
        "        # Linear output (no activation)\n",
        "        y_pred = sum([x[j] * weights[j] for j in range(3)]) + bias\n",
        "\n",
        "        # Error\n",
        "        error = y_true - y_pred\n",
        "\n",
        "        # Update weights and bias\n",
        "        for j in range(3):\n",
        "            weights[j] += lr * error * x[j]\n",
        "        bias += lr * error\n",
        "\n",
        "        total_error += error ** 2  # squared error\n",
        "\n",
        "    mse = total_error / len(X)\n",
        "    print(f\"Epoch {epoch+1}: MSE = {mse:.4f}\")\n",
        "\n",
        "# Final result\n",
        "print(\"\\nFinal Weights and Bias:\")\n",
        "print(f\"Weights: {weights}\")\n",
        "print(f\"Bias: {bias}\")\n"
      ],
      "metadata": {
        "id": "WXyF2_1vosu1",
        "outputId": "f31e7351-6f4a-4b9d-c20a-edcd020e54bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: MSE = 45.6556\n",
            "Epoch 2: MSE = 31.5164\n",
            "Epoch 3: MSE = 21.8400\n",
            "Epoch 4: MSE = 15.2160\n",
            "Epoch 5: MSE = 10.6797\n",
            "Epoch 6: MSE = 7.5713\n",
            "Epoch 7: MSE = 5.4398\n",
            "Epoch 8: MSE = 3.9764\n",
            "Epoch 9: MSE = 2.9702\n",
            "Epoch 10: MSE = 2.2768\n",
            "Epoch 11: MSE = 1.7975\n",
            "Epoch 12: MSE = 1.4647\n",
            "Epoch 13: MSE = 1.2324\n",
            "Epoch 14: MSE = 1.0687\n",
            "Epoch 15: MSE = 0.9523\n",
            "Epoch 16: MSE = 0.8681\n",
            "Epoch 17: MSE = 0.8063\n",
            "Epoch 18: MSE = 0.7597\n",
            "Epoch 19: MSE = 0.7238\n",
            "Epoch 20: MSE = 0.6952\n",
            "Epoch 21: MSE = 0.6716\n",
            "Epoch 22: MSE = 0.6517\n",
            "Epoch 23: MSE = 0.6343\n",
            "Epoch 24: MSE = 0.6187\n",
            "Epoch 25: MSE = 0.6045\n",
            "Epoch 26: MSE = 0.5912\n",
            "Epoch 27: MSE = 0.5787\n",
            "Epoch 28: MSE = 0.5668\n",
            "Epoch 29: MSE = 0.5554\n",
            "Epoch 30: MSE = 0.5443\n",
            "Epoch 31: MSE = 0.5336\n",
            "Epoch 32: MSE = 0.5232\n",
            "Epoch 33: MSE = 0.5131\n",
            "Epoch 34: MSE = 0.5032\n",
            "Epoch 35: MSE = 0.4936\n",
            "Epoch 36: MSE = 0.4842\n",
            "Epoch 37: MSE = 0.4750\n",
            "Epoch 38: MSE = 0.4660\n",
            "Epoch 39: MSE = 0.4572\n",
            "Epoch 40: MSE = 0.4485\n",
            "Epoch 41: MSE = 0.4401\n",
            "Epoch 42: MSE = 0.4319\n",
            "Epoch 43: MSE = 0.4238\n",
            "Epoch 44: MSE = 0.4159\n",
            "Epoch 45: MSE = 0.4082\n",
            "Epoch 46: MSE = 0.4006\n",
            "Epoch 47: MSE = 0.3932\n",
            "Epoch 48: MSE = 0.3859\n",
            "Epoch 49: MSE = 0.3788\n",
            "Epoch 50: MSE = 0.3718\n",
            "Epoch 51: MSE = 0.3650\n",
            "Epoch 52: MSE = 0.3584\n",
            "Epoch 53: MSE = 0.3518\n",
            "Epoch 54: MSE = 0.3454\n",
            "Epoch 55: MSE = 0.3392\n",
            "Epoch 56: MSE = 0.3330\n",
            "Epoch 57: MSE = 0.3270\n",
            "Epoch 58: MSE = 0.3211\n",
            "Epoch 59: MSE = 0.3154\n",
            "Epoch 60: MSE = 0.3097\n",
            "Epoch 61: MSE = 0.3042\n",
            "Epoch 62: MSE = 0.2988\n",
            "Epoch 63: MSE = 0.2935\n",
            "Epoch 64: MSE = 0.2883\n",
            "Epoch 65: MSE = 0.2832\n",
            "Epoch 66: MSE = 0.2782\n",
            "Epoch 67: MSE = 0.2733\n",
            "Epoch 68: MSE = 0.2686\n",
            "Epoch 69: MSE = 0.2639\n",
            "Epoch 70: MSE = 0.2593\n",
            "Epoch 71: MSE = 0.2548\n",
            "Epoch 72: MSE = 0.2504\n",
            "Epoch 73: MSE = 0.2461\n",
            "Epoch 74: MSE = 0.2418\n",
            "Epoch 75: MSE = 0.2377\n",
            "Epoch 76: MSE = 0.2336\n",
            "Epoch 77: MSE = 0.2296\n",
            "Epoch 78: MSE = 0.2257\n",
            "Epoch 79: MSE = 0.2219\n",
            "Epoch 80: MSE = 0.2181\n",
            "Epoch 81: MSE = 0.2145\n",
            "Epoch 82: MSE = 0.2109\n",
            "Epoch 83: MSE = 0.2073\n",
            "Epoch 84: MSE = 0.2039\n",
            "Epoch 85: MSE = 0.2005\n",
            "Epoch 86: MSE = 0.1972\n",
            "Epoch 87: MSE = 0.1939\n",
            "Epoch 88: MSE = 0.1907\n",
            "Epoch 89: MSE = 0.1876\n",
            "Epoch 90: MSE = 0.1845\n",
            "Epoch 91: MSE = 0.1815\n",
            "Epoch 92: MSE = 0.1785\n",
            "Epoch 93: MSE = 0.1756\n",
            "Epoch 94: MSE = 0.1728\n",
            "Epoch 95: MSE = 0.1700\n",
            "Epoch 96: MSE = 0.1673\n",
            "Epoch 97: MSE = 0.1646\n",
            "Epoch 98: MSE = 0.1620\n",
            "Epoch 99: MSE = 0.1594\n",
            "Epoch 100: MSE = 0.1569\n",
            "\n",
            "Final Weights and Bias:\n",
            "Weights: [2.3174632415345595, 2.9491929742109235, 0.2940334673091869]\n",
            "Bias: 4.181827326407938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perceptron for Linear Function with n Features**\n"
      ],
      "metadata": {
        "id": "XwRmWnCdpSzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def train_perceptron_linear(X, Y, lr=0.01, epochs=100):\n",
        "    n_features = len(X[0])\n",
        "    weights = [0.0] * n_features\n",
        "    bias = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_error = 0\n",
        "        for i in range(len(X)):\n",
        "            y_pred = sum(X[i][j] * weights[j] for j in range(n_features)) + bias\n",
        "            error = Y[i] - y_pred\n",
        "            # Update weights and bias\n",
        "            for j in range(n_features):\n",
        "                weights[j] += lr * error * X[i][j]\n",
        "            bias += lr * error\n",
        "            total_error += error ** 2\n",
        "        mse = total_error / len(X)\n",
        "        print(f\"Epoch {epoch+1}: MSE = {mse:.4f}\")\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "\n",
        "def generate_dataset(n_features, n_samples=10):\n",
        "    # Generate true random weights in [-1,1]\n",
        "    true_weights = [random.uniform(-1, 1) for _ in range(n_features)]\n",
        "    true_bias = 5  # fixed bias\n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "    for _ in range(n_samples):\n",
        "        features = [random.uniform(0, 1) for _ in range(n_features)]\n",
        "        y = sum(features[i] * true_weights[i] for i in range(n_features)) + true_bias\n",
        "        X.append(features)\n",
        "        Y.append(y)\n",
        "\n",
        "    return X, Y, true_weights, true_bias\n",
        "\n",
        "\n",
        "# Test for n=4 and n=5\n",
        "for n in [4, 5]:\n",
        "    print(f\"\\nTraining for n={n} features:\")\n",
        "    X, Y, true_w, true_b = generate_dataset(n)\n",
        "    print(f\"True weights: {true_w}\")\n",
        "    print(f\"True bias: {true_b}\")\n",
        "    learned_weights, learned_bias = train_perceptron_linear(X, Y)\n",
        "    print(f\"Learned weights: {learned_weights}\")\n",
        "    print(f\"Learned bias: {learned_bias}\")\n"
      ],
      "metadata": {
        "id": "3ZTuPaMro1vr",
        "outputId": "f89b6b59-cf35-486e-d56c-02bccff6520b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training for n=4 features:\n",
            "True weights: [-0.2589097947267678, 0.4629608102194671, -0.470054470771716, 0.33398240154739445]\n",
            "True bias: 5\n",
            "Epoch 1: MSE = 21.9498\n",
            "Epoch 2: MSE = 14.7610\n",
            "Epoch 3: MSE = 9.9923\n",
            "Epoch 4: MSE = 6.8273\n",
            "Epoch 5: MSE = 4.7251\n",
            "Epoch 6: MSE = 3.3276\n",
            "Epoch 7: MSE = 2.3972\n",
            "Epoch 8: MSE = 1.7769\n",
            "Epoch 9: MSE = 1.3622\n",
            "Epoch 10: MSE = 1.0841\n",
            "Epoch 11: MSE = 0.8968\n",
            "Epoch 12: MSE = 0.7698\n",
            "Epoch 13: MSE = 0.6830\n",
            "Epoch 14: MSE = 0.6230\n",
            "Epoch 15: MSE = 0.5809\n",
            "Epoch 16: MSE = 0.5507\n",
            "Epoch 17: MSE = 0.5286\n",
            "Epoch 18: MSE = 0.5119\n",
            "Epoch 19: MSE = 0.4988\n",
            "Epoch 20: MSE = 0.4883\n",
            "Epoch 21: MSE = 0.4794\n",
            "Epoch 22: MSE = 0.4718\n",
            "Epoch 23: MSE = 0.4649\n",
            "Epoch 24: MSE = 0.4587\n",
            "Epoch 25: MSE = 0.4528\n",
            "Epoch 26: MSE = 0.4473\n",
            "Epoch 27: MSE = 0.4421\n",
            "Epoch 28: MSE = 0.4370\n",
            "Epoch 29: MSE = 0.4321\n",
            "Epoch 30: MSE = 0.4273\n",
            "Epoch 31: MSE = 0.4226\n",
            "Epoch 32: MSE = 0.4180\n",
            "Epoch 33: MSE = 0.4135\n",
            "Epoch 34: MSE = 0.4090\n",
            "Epoch 35: MSE = 0.4047\n",
            "Epoch 36: MSE = 0.4004\n",
            "Epoch 37: MSE = 0.3961\n",
            "Epoch 38: MSE = 0.3919\n",
            "Epoch 39: MSE = 0.3878\n",
            "Epoch 40: MSE = 0.3838\n",
            "Epoch 41: MSE = 0.3798\n",
            "Epoch 42: MSE = 0.3758\n",
            "Epoch 43: MSE = 0.3719\n",
            "Epoch 44: MSE = 0.3681\n",
            "Epoch 45: MSE = 0.3643\n",
            "Epoch 46: MSE = 0.3605\n",
            "Epoch 47: MSE = 0.3568\n",
            "Epoch 48: MSE = 0.3532\n",
            "Epoch 49: MSE = 0.3496\n",
            "Epoch 50: MSE = 0.3460\n",
            "Epoch 51: MSE = 0.3425\n",
            "Epoch 52: MSE = 0.3391\n",
            "Epoch 53: MSE = 0.3356\n",
            "Epoch 54: MSE = 0.3323\n",
            "Epoch 55: MSE = 0.3289\n",
            "Epoch 56: MSE = 0.3256\n",
            "Epoch 57: MSE = 0.3224\n",
            "Epoch 58: MSE = 0.3192\n",
            "Epoch 59: MSE = 0.3160\n",
            "Epoch 60: MSE = 0.3128\n",
            "Epoch 61: MSE = 0.3098\n",
            "Epoch 62: MSE = 0.3067\n",
            "Epoch 63: MSE = 0.3037\n",
            "Epoch 64: MSE = 0.3007\n",
            "Epoch 65: MSE = 0.2977\n",
            "Epoch 66: MSE = 0.2948\n",
            "Epoch 67: MSE = 0.2920\n",
            "Epoch 68: MSE = 0.2891\n",
            "Epoch 69: MSE = 0.2863\n",
            "Epoch 70: MSE = 0.2835\n",
            "Epoch 71: MSE = 0.2808\n",
            "Epoch 72: MSE = 0.2781\n",
            "Epoch 73: MSE = 0.2754\n",
            "Epoch 74: MSE = 0.2728\n",
            "Epoch 75: MSE = 0.2702\n",
            "Epoch 76: MSE = 0.2676\n",
            "Epoch 77: MSE = 0.2651\n",
            "Epoch 78: MSE = 0.2625\n",
            "Epoch 79: MSE = 0.2601\n",
            "Epoch 80: MSE = 0.2576\n",
            "Epoch 81: MSE = 0.2552\n",
            "Epoch 82: MSE = 0.2528\n",
            "Epoch 83: MSE = 0.2504\n",
            "Epoch 84: MSE = 0.2481\n",
            "Epoch 85: MSE = 0.2457\n",
            "Epoch 86: MSE = 0.2435\n",
            "Epoch 87: MSE = 0.2412\n",
            "Epoch 88: MSE = 0.2390\n",
            "Epoch 89: MSE = 0.2368\n",
            "Epoch 90: MSE = 0.2346\n",
            "Epoch 91: MSE = 0.2324\n",
            "Epoch 92: MSE = 0.2303\n",
            "Epoch 93: MSE = 0.2282\n",
            "Epoch 94: MSE = 0.2261\n",
            "Epoch 95: MSE = 0.2240\n",
            "Epoch 96: MSE = 0.2220\n",
            "Epoch 97: MSE = 0.2200\n",
            "Epoch 98: MSE = 0.2180\n",
            "Epoch 99: MSE = 0.2160\n",
            "Epoch 100: MSE = 0.2141\n",
            "Learned weights: [0.2058846038800565, 1.4917576092250608, 0.6591443945055817, 0.8899590392973771]\n",
            "Learned bias: 3.2859247717073226\n",
            "\n",
            "Training for n=5 features:\n",
            "True weights: [-0.7930039759022096, 0.15432909935206962, -0.7765906638143925, 0.9070315769595649, -0.9633719448898668]\n",
            "True bias: 5\n",
            "Epoch 1: MSE = 14.4158\n",
            "Epoch 2: MSE = 9.2789\n",
            "Epoch 3: MSE = 6.0480\n",
            "Epoch 4: MSE = 4.0151\n",
            "Epoch 5: MSE = 2.7351\n",
            "Epoch 6: MSE = 1.9283\n",
            "Epoch 7: MSE = 1.4188\n",
            "Epoch 8: MSE = 1.0960\n",
            "Epoch 9: MSE = 0.8906\n",
            "Epoch 10: MSE = 0.7589\n",
            "Epoch 11: MSE = 0.6736\n",
            "Epoch 12: MSE = 0.6173\n",
            "Epoch 13: MSE = 0.5794\n",
            "Epoch 14: MSE = 0.5530\n",
            "Epoch 15: MSE = 0.5339\n",
            "Epoch 16: MSE = 0.5194\n",
            "Epoch 17: MSE = 0.5079\n",
            "Epoch 18: MSE = 0.4983\n",
            "Epoch 19: MSE = 0.4899\n",
            "Epoch 20: MSE = 0.4824\n",
            "Epoch 21: MSE = 0.4756\n",
            "Epoch 22: MSE = 0.4691\n",
            "Epoch 23: MSE = 0.4630\n",
            "Epoch 24: MSE = 0.4571\n",
            "Epoch 25: MSE = 0.4515\n",
            "Epoch 26: MSE = 0.4460\n",
            "Epoch 27: MSE = 0.4407\n",
            "Epoch 28: MSE = 0.4356\n",
            "Epoch 29: MSE = 0.4307\n",
            "Epoch 30: MSE = 0.4258\n",
            "Epoch 31: MSE = 0.4212\n",
            "Epoch 32: MSE = 0.4166\n",
            "Epoch 33: MSE = 0.4122\n",
            "Epoch 34: MSE = 0.4079\n",
            "Epoch 35: MSE = 0.4037\n",
            "Epoch 36: MSE = 0.3996\n",
            "Epoch 37: MSE = 0.3956\n",
            "Epoch 38: MSE = 0.3917\n",
            "Epoch 39: MSE = 0.3880\n",
            "Epoch 40: MSE = 0.3843\n",
            "Epoch 41: MSE = 0.3807\n",
            "Epoch 42: MSE = 0.3772\n",
            "Epoch 43: MSE = 0.3738\n",
            "Epoch 44: MSE = 0.3705\n",
            "Epoch 45: MSE = 0.3672\n",
            "Epoch 46: MSE = 0.3640\n",
            "Epoch 47: MSE = 0.3609\n",
            "Epoch 48: MSE = 0.3579\n",
            "Epoch 49: MSE = 0.3549\n",
            "Epoch 50: MSE = 0.3520\n",
            "Epoch 51: MSE = 0.3492\n",
            "Epoch 52: MSE = 0.3464\n",
            "Epoch 53: MSE = 0.3437\n",
            "Epoch 54: MSE = 0.3410\n",
            "Epoch 55: MSE = 0.3384\n",
            "Epoch 56: MSE = 0.3358\n",
            "Epoch 57: MSE = 0.3333\n",
            "Epoch 58: MSE = 0.3308\n",
            "Epoch 59: MSE = 0.3284\n",
            "Epoch 60: MSE = 0.3260\n",
            "Epoch 61: MSE = 0.3237\n",
            "Epoch 62: MSE = 0.3214\n",
            "Epoch 63: MSE = 0.3191\n",
            "Epoch 64: MSE = 0.3169\n",
            "Epoch 65: MSE = 0.3147\n",
            "Epoch 66: MSE = 0.3126\n",
            "Epoch 67: MSE = 0.3105\n",
            "Epoch 68: MSE = 0.3084\n",
            "Epoch 69: MSE = 0.3064\n",
            "Epoch 70: MSE = 0.3044\n",
            "Epoch 71: MSE = 0.3024\n",
            "Epoch 72: MSE = 0.3004\n",
            "Epoch 73: MSE = 0.2985\n",
            "Epoch 74: MSE = 0.2966\n",
            "Epoch 75: MSE = 0.2948\n",
            "Epoch 76: MSE = 0.2929\n",
            "Epoch 77: MSE = 0.2911\n",
            "Epoch 78: MSE = 0.2893\n",
            "Epoch 79: MSE = 0.2875\n",
            "Epoch 80: MSE = 0.2858\n",
            "Epoch 81: MSE = 0.2841\n",
            "Epoch 82: MSE = 0.2824\n",
            "Epoch 83: MSE = 0.2807\n",
            "Epoch 84: MSE = 0.2790\n",
            "Epoch 85: MSE = 0.2774\n",
            "Epoch 86: MSE = 0.2757\n",
            "Epoch 87: MSE = 0.2741\n",
            "Epoch 88: MSE = 0.2726\n",
            "Epoch 89: MSE = 0.2710\n",
            "Epoch 90: MSE = 0.2694\n",
            "Epoch 91: MSE = 0.2679\n",
            "Epoch 92: MSE = 0.2664\n",
            "Epoch 93: MSE = 0.2649\n",
            "Epoch 94: MSE = 0.2634\n",
            "Epoch 95: MSE = 0.2619\n",
            "Epoch 96: MSE = 0.2604\n",
            "Epoch 97: MSE = 0.2590\n",
            "Epoch 98: MSE = 0.2576\n",
            "Epoch 99: MSE = 0.2562\n",
            "Epoch 100: MSE = 0.2548\n",
            "Learned weights: [0.06126891890961634, 0.8958312881582295, 0.9531184234394345, 1.3327625635451958, 0.3014210022580652]\n",
            "Learned bias: 2.3289598213126785\n"
          ]
        }
      ]
    }
  ]
}